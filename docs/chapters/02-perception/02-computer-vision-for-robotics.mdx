---
title: "Lesson 2.2: Computer Vision for Robotics"
sidebar_label: "Computer Vision for Robotics"
description: "Object detection, recognition, and tracking in robotic contexts, depth estimation, and visual servoing for humanoid robots"
keywords: ["computer vision", "robotics vision", "object detection", "visual servoing"]
---

# Lesson 2.2: Computer Vision for Robotics

## Introduction

Computer vision is a critical component of humanoid robotics, enabling robots to perceive and interpret visual information from their environment. This lesson explores the application of computer vision techniques specifically tailored for robotic systems, including object detection, recognition, tracking, and visual servoing for precise manipulation tasks.

## Learning Objectives

After completing this lesson, students will be able to:
- Understand the fundamentals of computer vision in robotic applications
- Implement basic object detection and recognition algorithms
- Apply visual servoing techniques for robot control

## Prerequisites

Students should have knowledge of:
- Basic understanding of image processing concepts
- Programming skills in Python and OpenCV
- Familiarity with linear algebra concepts

## Theoretical Foundation

Computer vision in robotics differs from traditional computer vision in that it must operate in real-time, handle dynamic environments, and integrate seamlessly with robot control systems. The visual information must be processed to support navigation, manipulation, and interaction tasks.

### Key Concepts

- **Visual Servoing**: Using visual feedback to control robot motion
- **Real-time Processing**: Processing images at frame rates suitable for robot control
- **3D Reconstruction**: Estimating depth and 3D structure from 2D images

## Practical Application

Let's explore computer vision techniques specifically for robotics applications:

### Example 1: Basic Object Detection for Robotics

```python
import cv2
import numpy as np
import time

class RobotVisionSystem:
    def __init__(self):
        self.object_templates = {}
        self.camera_matrix = None  # Camera intrinsic parameters
        self.dist_coeffs = None    # Distortion coefficients

    def detect_objects(self, image):
        """Detect objects in the image and return their positions"""
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Apply Gaussian blur to reduce noise
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)

        # Use Canny edge detection
        edges = cv2.Canny(blurred, 50, 150)

        # Find contours
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        objects = []
        for contour in contours:
            # Filter by area to avoid tiny contours
            if cv2.contourArea(contour) > 100:
                # Get bounding box
                x, y, w, h = cv2.boundingRect(contour)

                # Calculate center
                center_x = x + w // 2
                center_y = y + h // 2

                # Approximate contour to polygon
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)

                # Determine object type based on shape
                if len(approx) == 3:
                    obj_type = "triangle"
                elif len(approx) == 4:
                    # Check if it's square or rectangle
                    aspect_ratio = float(w) / h
                    if 0.9 <= aspect_ratio <= 1.1:
                        obj_type = "square"
                    else:
                        obj_type = "rectangle"
                else:
                    obj_type = "circle"  # Approximate other shapes as circles

                objects.append({
                    'type': obj_type,
                    'center': (center_x, center_y),
                    'bbox': (x, y, w, h),
                    'contour': contour
                })

        return objects

    def estimate_depth(self, left_image, right_image):
        """Estimate depth using stereo vision"""
        # Convert to grayscale
        gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)
        gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)

        # Create stereo block matcher
        stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)

        # Compute disparity map
        disparity = stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0

        # Convert disparity to depth (simplified)
        # In practice, you'd use camera calibration parameters
        baseline = 0.1  # Camera baseline in meters
        focal_length = 1000  # Focal length in pixels (example)
        depth_map = (baseline * focal_length) / (disparity + 1e-6)  # Add small value to avoid division by zero

        return depth_map

# Example usage
vision_system = RobotVisionSystem()

# Simulate capturing an image (in practice, this would come from a camera)
# For demonstration, we'll create a synthetic image
synthetic_image = np.zeros((480, 640, 3), dtype=np.uint8)
cv2.rectangle(synthetic_image, (200, 150), (300, 250), (0, 255, 0), 2)  # Green rectangle
cv2.circle(synthetic_image, (400, 200), 50, (255, 0, 0), 2)  # Blue circle

# Detect objects
detected_objects = vision_system.detect_objects(synthetic_image)

print(f"Detected {len(detected_objects)} objects:")
for obj in detected_objects:
    print(f"  Type: {obj['type']}, Center: {obj['center']}, BBox: {obj['bbox']}")

# Draw bounding boxes on image
for obj in detected_objects:
    x, y, w, h = obj['bbox']
    cv2.rectangle(synthetic_image, (x, y), (x+w, y+h), (0, 255, 255), 2)
    cv2.putText(synthetic_image, obj['type'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
```

### Example 2: Visual Servoing for Robot Control

```python
import numpy as np
import cv2

class VisualServoingController:
    def __init__(self, target_x=320, target_y=240, tolerance=10):
        self.target_x = target_x  # Target x-coordinate (center of image)
        self.target_y = target_y  # Target y-coordinate (center of image)
        self.tolerance = tolerance  # Tolerance for reaching target
        self.gain = 0.01  # Control gain

    def calculate_control_commands(self, object_position):
        """Calculate robot control commands based on object position"""
        obj_x, obj_y = object_position

        # Calculate errors
        error_x = self.target_x - obj_x
        error_y = self.target_y - obj_y

        # Check if object is within tolerance
        if abs(error_x) < self.tolerance and abs(error_y) < self.tolerance:
            return {
                'command': 'REACHED_TARGET',
                'linear_x': 0,
                'linear_y': 0,
                'angular_z': 0
            }

        # Calculate control commands (simplified)
        linear_x = self.gain * error_y  # Move forward/backward based on y-error
        linear_y = self.gain * error_x  # Move left/right based on x-error
        angular_z = self.gain * (-error_x)  # Rotate based on x-error

        # Limit maximum velocities
        max_vel = 0.5
        linear_x = max(min(linear_x, max_vel), -max_vel)
        linear_y = max(min(linear_y, max_vel), -max_vel)
        angular_z = max(min(angular_z, max_vel), -max_vel)

        return {
            'command': 'MOVE_TO_OBJECT',
            'linear_x': linear_x,
            'linear_y': linear_y,
            'angular_z': angular_z,
            'error_x': error_x,
            'error_y': error_y
        }

    def track_and_control(self, image, object_position):
        """Track object and generate control commands"""
        # Calculate control commands
        commands = self.calculate_control_commands(object_position)

        # Draw target crosshair on image
        cv2.line(image, (self.target_x - 20, self.target_y), (self.target_x + 20, self.target_y), (0, 255, 0), 2)
        cv2.line(image, (self.target_x, self.target_y - 20), (self.target_x, self.target_y + 20), (0, 255, 0), 2)

        # Draw object position
        cv2.circle(image, object_position, 10, (255, 0, 0), 2)

        # Draw line from object to target
        cv2.line(image, object_position, (self.target_x, self.target_y), (255, 255, 0), 1)

        return commands, image

# Example usage
servo_controller = VisualServoingController()

# Simulate an image with an object at a specific position
simulated_image = np.zeros((480, 640, 3), dtype=np.uint8)
object_pos = (280, 200)  # Object position in image

# Apply visual servoing
commands, annotated_image = servo_controller.track_and_control(simulated_image, object_pos)

print(f"Control commands: {commands}")
```

## Implementation Guide

### Step 1: Setup

Install necessary computer vision libraries:

```bash
pip install opencv-python numpy
```

### Step 2: Implementation

1. Set up camera interface and calibration
2. Implement object detection algorithms
3. Create visual servoing control loop
4. Add 3D reconstruction capabilities

### Step 3: Testing

Validate your implementation by:
- Testing object detection with various lighting conditions
- Verifying visual servoing accuracy
- Measuring processing frame rates

## Hands-on Exercise

Implement a basic computer vision system for robotics:

1. **Task 1**: Create an object detection system for simple geometric shapes
2. **Task 2**: Implement a visual servoing controller to track objects
3. **Task 3**: Add depth estimation using stereo vision (simulated)

### Exercise Requirements

- Support detection of at least 3 different object types
- Implement real-time processing (30+ FPS)
- Include basic visual servoing for object tracking

## Verification

How to test and validate the implementation:

- Test with various objects and lighting conditions
- Verify that visual servoing achieves accurate positioning
- Measure processing latency and throughput

## Troubleshooting Guide

Common issues and solutions:

- **Issue 1**: Object detection failing in varying lighting conditions
  - Solution: Implement adaptive thresholding and preprocessing
- **Issue 2**: Visual servoing oscillating around target
  - Solution: Adjust control gains and add damping
- **Issue 3**: High computational load affecting real-time performance
  - Solution: Optimize algorithms and consider hardware acceleration

## Real-World Relevance

Computer vision is essential in humanoid robotics for:
- Navigation and obstacle avoidance
- Object recognition and manipulation
- Human-robot interaction
- Environmental mapping and localization

## Safety Considerations

When working with computer vision systems:
- Ensure proper camera calibration for accurate measurements
- Implement fallback behaviors when vision fails
- Consider the impact of lighting and environmental conditions

## Further Exploration

Advanced topics and additional resources for students who want to dive deeper:

- Study of deep learning approaches for object detection (YOLO, SSD)
- Research into 3D object pose estimation
- Exploration of SLAM algorithms for mapping
- Investigation of event-based vision systems

## Summary

This lesson covered the fundamentals of computer vision in robotics, including object detection, recognition, and visual servoing techniques specifically tailored for humanoid robot applications.

## Knowledge Check

- Question 1: How does computer vision in robotics differ from traditional computer vision?
- Question 2: What is visual servoing and how is it used in robotics?
- Question 3: What are the challenges of real-time computer vision for robots?